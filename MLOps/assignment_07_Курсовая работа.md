# Отчет: Проектирование рекомендательной системы для сайта электронной коммерции
 
**Курс:** Проектирование систем машинного обучения
**Студент:** Горин Андрей Валерьевич
**Группа:** М8О - 110
 

## 1. Введение и постановка задачи
 
Проектируется система для сайта электронной коммерции, которая в реальном времени рекомендовать следующий товар, на который пользователь с наибольшей вероятностью кликнет в рамках текущей сессии, опираясь на краткосрочное намерение, а не на долгосрочный профиль пользователя.
**Бизнес-цели:**
•	**CTR@K (Click-Through Rate):** Увеличение доли рекомендаций, по которым пользователи кликают до 8-12%.
•	**Глубина сессии (Session Depth):** Увеличение количества страниц или товаров, которые пользователь посмотрел за визит до 4-6.
•	**Доход на сессию (Revenue per Session):** Рост среднего дохода от одного визита на 5-10%.
**Требования к системе:**
•	**Задержка (latency)** ответа не должна превышать **121 мс**.
•	Система должна обслуживать **4,860,532 активных пользователей в день (DAU)**, с пиковой нагрузкой в **5,795 запросов в секунду (RPS)**.
•	Система должна быть масштабируемой и отказоустойчивой.
 

## Часть 1: Формулировка ML-задачи и выбор модели
 
**1. Определение ML-задачи**
Задачу можно сформулировать как **задача ранжирования (Learning to Rank) с элементами sequence-modeling**. Система не «угадывает один правильный товар», а формирует список товаров и сортирует его так, чтобы самые вероятные для клика оказались вверху.
**Входные данные:**
**Сессионные данные (основные):**
•	последовательность item_id;
•	тип действия (просмотр, клик, добавление в корзину);
•	временные интервалы между событиями,
•	позиция товара в листинге.
**Контекстные признаки:**
•	устройство (mobile / desktop),
•	источник трафика,
•	время суток / день недели.
**Признаки товаров:**
•	категория,
•	бренд,
•	цена,
•	embedding товара (предварительно вычесленный).
**Источники данных:**
•	Журнал событий,
•	Каталог товаров,
•	Централизованное хранилище признаков (online + offline).
**Выходные данные:** ранжированная последовательность товаров.
**Целевая переменная (таргет):** факт клика пользователя по товару.
Для пары (сессия, товар-кандидат):
•	Target = 1, если пользователь кликнул на товар следующим
•	Target = 0, если товар был показан, но не выбран

**2. Выбор модели**
Рассмотрим три подхода:

**Модель 1:**
**GRU4Rec / Session-based RNN**
**Описание:** Рекуррентная нейросеть (GRU), обучаемая на последовательностях товаров внутри сессии.
**Преимущества:**
•	хорошо моделирует краткосрочные намерения,
•	не требует user_id,
•	высокая точность для session-based задач.
**Недостатки:**
•	относительно высокая latency,
•	сложнее масштабировать online-inference,
•	менее интерпретируема.

**Модель 2:**
**Transformer-based Session Model (SASRec / BERT4Rec)**
**Описание:** Self-attention модель для предсказания следующего item на основе последовательности.
**Преимущества:**
•	лучше захватывает long-range зависимости,
•	state-of-the-art качество.
**Недостатки:**
•	высокая вычислительная сложность,
•	риск не уложиться в SLA,
•	требует агрессивной оптимизации.

**Модель 3 (выбранная):**
**Двухэтапная рекомендательная система (Two-Stage Recommendation):**
**Candidate Generation + LightGBM Ranker**
**Описание:** двухэтапная система, где сначала быстро отбирается набор потенциально интересных товаров-кандидатов, а затем модель LightGBM ранжирует их по вероятности клика, чтобы показать пользователю наиболее релевантные позиции.
**Этап 1 — генерация кандидатов:**
•	Совместное посещение (co-visitation),
•	Векторные представления элементов (item-to-item embeddings),
•	Эвристики на основе популярности (popularity-based heuristics).
**Этап 2 — ранжирование:**
•	LightGBM Ranker / XGBoost Ranker.
За счет двухэтапности система сначала быстро отбирает небольшой список подходящих товаров (например, 100 – 300 штук), а затем более внимательно сортирует их. Это делается потому, что проверять сразу весь каталог из сотен тысяч товаров слишком долго и дорого.
**Преимущества:**
•	гарантированно низкая latency,
•	легко масштабируется,
•	прозрачна для бизнеса,
•	удобно обновлять online-фичи.
**Недостатки:**
•	качество ниже end-to-end deep-models (компенсируется ансамблями).

**Выбор:** Для данной задачи выбираем **Two-Stage Recommendation с GBDT-ранжированием**, потому что эта архитектура сочетает достаточную точность рекомендаций с высокой скоростью и масштабируемостью, что критично для онлайн-системы с миллионами пользователей и ограничением latency ≤121 мс.
 

## Часть 2: Проектирование архитектуры
 
**1. Высокоуровневая архитектура системы**
Высокоуровневая архитектура показывает взаимодействие всех основных компонентов системы: от сбора данных до развертывания обученной модели и мониторинга.
**Ключевые компоненты:**
**Web / Mobile Client:** Приложения и веб-сайт, через которые пользователи взаимодействуют с платформой и делают клики по товарам.
**API Gateway (Envoy Gateway):** Сервис, принимающий все входящие запросы от клиентов и маршрутизирующий их к внутренним сервисам.
**Load Balancer (MetalLB (L2/BGP) + Ingress (NGINX/Envoy)):** Распределяет входящий трафик между экземплярами Recommendation Service для обеспечения отказоустойчивости и соблюдения SLA по задержке ответа.
**Cache Layer (Redis Cluster):** Кэширует недавно рассчитанные рекомендации для ускорения повторных запросов в той же сессии.
**Recommendation Service:** Основной сервис, собирающий рекомендации, объединяющий кандидатов и результаты ранжирования, применяющий бизнес-правила и fallback-логики. Сервис использует online-признаки, embeddings и версии моделей из Model Registry.
**Online Feature Store (Redis / RocksDB):** Быстрое хранилище актуальных online-признаков пользователей, товаров и сессий с TTL, используемых во время инференса.
**Candidate Generator (FAISS):** Компонент, формирующий список потенциально интересных товаров (100–300) для дальнейшего ранжирования.
**Ranking Model Service:** Сервис, содержащий модель LightGBM для сортировки кандидатов по вероятности клика и embeddings.
**Event Logging:** Сервис для сбора и передачи событий пользователя (клики, просмотры) в систему хранения и анализа для дальнейшей обработки, обучения моделей и аналитики.
**Data Lake (S3 / HDFS):** Централизованное хранилище всех исторических данных о пользователях, товарах, сессиях обучающих выборок и embeddings для аналитики и обучения моделей.
**Training Platform:** Автоматизированная платформа для подготовки данных, обучения и переобучения, валидации качества, регистрации и деплоя моделей Candidate Generator и Ranking Model.
**Model Registry (MLflow):** Система управления моделями, обеспечивающая: версионирование моделей, хранение артефактов, контроль используемых моделей в продакшен; поддержку canary deployment.
**Monitoring & Logging (Prometheus, ELK):** Система мониторинга latency (SLA ≤ 121 мс), ошибок, качества рекомендаций, data drift и model drift.

**2 Архитектура Data Pipeline**
Data Pipeline описывает процесс сбора, разметки и подготовки данных для обучения моделей.
**Источники данных:**
**User Events (Kafka topics):** Распределённая очередь сообщений с событиями пользователей, такими как клики и просмотры товаров.
**Product Updates:** Источник обновлений информации о товарах, включая описание и категорию.
**Pricing / Availability:** Источник данных о цене и наличии товаров.
**Этапы:**
1.	**Сбор данных (Kafka → Stream Processor (Flink)):** Данные из Kafka поступают напрямую в потоковый процессор, который обрабатывающий события в реальном времени;
2.	**Очистка, дедупликация (Apache Spark):** Модуль удаления дубликатов и исправления некорректных данных.
3.	**Feature extraction (Flink):** Вычисление признаков пользователей и товаров для модели.
4.	**Запись:**
    •	**Offline Store (S3 / HDFS):** Хранилище для долговременного хранения исторических данных, обучающих выборок и embeddings;
    •	**Online Store (Redis / RocksDB):** Быстрое хранилище для актуальных признаков, используемых во время онлайн-рекомендаций и embeddings с TTL.
**Выходы:**
    •	обучающие датасеты для всех моделей;
    •	online-признаки;
    •	pre-computed embeddings;
    •	лейблы для обучения моделей ранжирования.

**3 Архитектура Training Pipeline**
Training Pipeline показывает автоматизированный процесс обучения моделей с триггерами, валидацией и развертыванием.
**Процесс:**
1.	**Триггер:** Pipeline запускается автоматически по расписанию или при необходимости обновления модели.
2.	**Подготовка данных:**
    •	**Data Lake → Feature Join:** Данные пользователей и товаров объединяются из централизованного хранилища для подготовки обучающих наборов признаков.
    •	**Session reconstruction (Flink):** Воссоздание последовательностей действий пользователей в сессии для обучения модели на правильном порядке событий.
    •  	**Label Generation (Spark/SQL):** Формирование целевой переменной (клик / не клик).
    •	**Negative sampling (Apache Spark):** Генерация «негативных примеров» (товаров, на которые пользователь не кликнул) для обучения модели различать релевантные и нерелевантные товары.
3.	**Обучение (Model training):**
    •	**Candidate Generator Model Training:** обучение моделей генерации кандидатов и построение ANN-индексов;
    •	**Ranking Model Training:** обучение LightGBM Ranker для предсказания вероятности клика.
4.	**Валидация (Offline evaluation):** Проверка качества моделей с помощью метрик, отражающих насколько правильно модели отбирают и ранжируют кандидатов (NDCG@K, MRR Recall@K (для Сandidate Generator)).
5.	**Регистрация (Model registry):** Регистрация успешной модели в системе управления моделями для версионирования и последующего использования.
6.	**Развертывание (Canary deployment):** Пошаговое развертывание модели в продакшн, сначала для небольшой части пользователей (например, 10%) с мониторингом метрик, перед полной заменой старой модели.

**4 Архитектура Inference Pipeline (Serving)**
**Компоненты:**
    •	**Load Balancer (MetalLB (L2/BGP) + Ingress (NGINX/Envoy)):** Распределяет входящие запросы от Web/Mobile клиентов между экземплярами Recommendation Service для равномерной нагрузки.
    •	**Recommendation Service Pods:** Каждый под содержит Candidate Generator и Ranking Model (LightGBM) для генерации и ранжирования рекомендаций в формате, готовом к онлайн-серверу.
    •	**HPA (Horizontal Pod Autoscaler):** Автоматически масштабирует количество подов Recommendation Service при увеличении RPS, чтобы выдерживать пиковые нагрузки.
    •	**Online Feature Store (Redis / RocksDB):** Быстрое хранилище признаков пользователей и товаров, к которым обращается модель во время инференса.
    •	**Cache Layer (Redis Cluster):** Кэширует недавно рассчитанные рекомендации для ускорения повторных запросов в той же сессии.
    •	**Event Logging / Kafka:** Отправляет события пользователя (клики, просмотры) в систему сбора данных для дальнейшего обучения и аналитики.
    •	**Monitoring & Logging (Prometheus & ELK):** Мониторинг метрик системы, логирование ошибок и задержек, контроль SLA (≤121 мс).
    •	**Canary Deployment (Argo Rollouts):** Новые версии моделей или бизнес-логики сначала тестируются на небольшой доле трафика (например, 10%) перед полной заменой.
**Онлайн шаги:**
    1.	Получение контекста текущей сессии (Redis)
    2.	Генерация 100–300 товаров-кандидатов (Candidate Generator)
    3.	Получение online-признаков и embeddings
    4.	Ранжирование кандидатов моделью LightGBM
    5.	Применение бизнес-правил и fallback-логики
    6.	Отбор Top-K товаров (например, 10–20) и возврат ответа клиенту
    7.	Логирование событий (показы, клики)
**Оптимизации:**
    •	**Pre-computed Embeddings:** Заранее вычисленные числовые представления товаров и пользователей для быстрого поиска похожих объектов без онлайн-вычислений.
    •	**Batch-Inference:** Модуль массового предсказания рекомендаций для большого числа пользователей одновременно, снижая нагрузку и ускоряя обновление.
    •	**Model Warm-Up:** Процесс «разогрева» модели перед онлайн-запросами, чтобы первый запрос не задерживался и система сразу отвечала быстро.
    •	**Cache Layer:** Ускорение повторных запросов;
    •	**Fallback-логика:** Запасной механизм, который возвращает базовые рекомендации (например, популярные товары), если основная модель недоступна или ответ слишком медленный.

## Часть 3. Расчёты и нефункциональные требования
 
**1 Расчёт требований к хранилищу**
**События:**
В e-commerce выделяют следующие типы событий:
**Тип события**                 **Пример**
    page_view             просмотр категории / PDP
    item_view	             просмотр товара
    click                    клик по карточке
    search                   поисковый запрос
    add_to_cart            добавление в корзину
    remove_from_cart	        удаление
    checkout	           переход к оформлению
    heartbeat                активность сессии
**Типичная структура сессии среднего пользователя:**
    •	2–4 сессии в день
    •	10–15 действий на сессию
Итого диапазон от (2х10) 20 до (4х15) 60 событий в день. Середина диапазона 40. Чтобы не допустить занижение нагрузки, консервативно допускаем 50 событий на одного пользователя в день.
**Размеры (приближённо) событие клика в JSON:**
**Поле**	    **Байты**
ключи JSON	      ~60
значения	      ~80
id / timestamp	  ~40
служебные поля	  ~20
**Итого**	    **~200**
**Таким образом получим следующие требования к хранилищу:**
    •	4.86 млн DAU
    •	~50 событий / пользователь
    •	200 байт / событие
4.86M × 50 × 200B ≈ 48.6 GB / день
**Хранение за год:**
≈ 18 ТБ (с репликацией ≈ 36 ТБ).

**2 Пропускная способность (Throughput)**
**Требуемая пропускная способность:** 5,795 RPS
**Производительность одного инстанса:**
Один инстанс на 4 vCPU может обрабатывать ~ 80 – 120 RPS. Для пиковой нагрузки возьмем среднюю производительность.
Количество инстансов:
5,795 RPS ÷ 100 RPS/инстанс ≈ 58 инстансов
С учетом отказоустойчивости: 58 х 1.2 ≈ 70 инстансов
**Требования к сети:**
Размер ответа, типичного для e-commerce API, соответствует передаче 15–20 рекомендаций в JSON без медиаконтента и составит порядка 5 КБ.
Трафик: 5,795 × 5KB ≈ 28.9 MБ/s
С запасом ≈ **50 MБ/s**.

**3 Задержки (Latency Budget)**
Этап	                Время
Network + Gateway	    20 мс
Candidate Generation	25 мс
Feature Fetch	        30 мс
Ranking	                25 мс
Serialization	        10 мс
**Итого**	          **110 мс**
SLA соблюдён.

**4 Масштабируемость и надёжность**
**Масштабируемость:**
    **•	Горизонтальное масштабирование:** Inference Service развернут как stateless сервис в Kubernetes и может автоматически масштабироваться с помощью HPA при увеличении нагрузки (например, CPU > 70% или по RPS).
    **•	Масштабирование данных:** Redis sharded-кластер обеспечивает распределённое хранение и быстрый доступ к признакам, а Kafka и S3 легко масштабируются для обработки потоков событий и хранения исторических данных.
    **•	Масштабирование обучения:** При необходимости обучение моделей можно масштабировать на несколько CPU-нод или кластер с распределённым обучением для параллельного обучения нескольких моделей или обработки больших исторических данных.
**Надёжность:**
    **•	Репликация и отказоустойчивость:** Несколько реплик Inference Service в Kubernetes обеспечивают отказоустойчивость; Multi-AZ deployment гарантирует устойчивость к сбоям зоны доступности.
    **•	Защита от перегрузок:** Circuit Breakers предотвращают каскадные отказы при недоступности внешних сервисов.
    **•	Резервные варианты:** В случае проблем с моделью предоставляются fallback recommendations.
    **•	Health Checks и мониторинг:** Настроены liveness и readiness пробы для автоматического перезапуска нездоровых подов; возможно использование систем мониторинга и алертинга для отслеживания состояния сервисов.
    **•	Восстановление после сбоев:** Kubernetes обеспечивает автоматическое восстановление подов и балансировку нагрузки после отказов узлов.
**Доступность:**
    **•	Target SLA:** ≥ 99.9%, обеспечивается за счёт репликации, Multi-AZ deployment, горизонтального масштабирования и механизмов отказоустойчивости.
 

## Список использованных источников
 
*1.	Hidasi et al., Session-based Recommendations with RNNs, ICLR*
*2.	Kang & McAuley, Self-Attentive Sequential Recommendation, ICDM*
*3.	Covington et al., Deep Neural Networks for YouTube Recommendations*
*4.	LightGBM Documentation**
*5.	Designing Data-Intensive Applications — Martin Kleppmann*
*6.	Google ML System Design Docs*







# Домашнее задание: Проектирование ML-системы — Задание 7

**Курс:** Проектирование систем машинного обучения

**Уровень:** 2 курс магистратуры

**Задание 7**

## 1. Введение и постановка задачи

7. **Рекомендация следующего товара в сессии:** Для сайта электронной коммерции спроектируйте систему, которая рекомендует следующий товар, на который пользователь, скорее всего, нажмет в рамках той же сессии. Сосредоточьтесь на краткосрочных намерениях пользователя и обновлениях с низкой задержкой.

**Бизнес-цели:**
- Определите 2-3 ключевые бизнес-метрики, которые ваша система должна улучшить (например, повышение точности, снижение затрат, увеличение вовлеченности пользователей).

**Требования к системе:**
- Задержка (latency) ответа не должна превышать **121 мс**.
- Система должна обслуживать **4,860,532 активных пользователей в день (DAU)**, с пиковой нагрузкой в **5,795 запросов в секунду (RPS)**.
- Система должна быть масштабируемой и отказоустойчивой.

## 2. Задание

Вам необходимо разработать комплексный проект ML-системы, который включает в себя следующие части:

### Часть 1: Формулировка ML-задачи и выбор модели (Мини-отчёт)

1.  **Определение ML-задачи:**
    - Как можно сформулировать задачу? (например, классификация, регрессия, ранжирование, генерация).
    - Какой будет таргет (целевая переменная)?
    - Какие данные вам понадобятся для обучения?

2.  **Выбор модели:**
    - Предложите как минимум две модели.
    - Обоснуйте свой выбор, указав преимущества и недостатки каждой.
    - Выберите одну модель для дальнейшего проектирования.

### Часть 2: Проектирование архитектуры (Диаграммы в draw.io)

1.  **Высокоуровневая архитектура системы.**
2.  **Архитектура Data Pipeline.**
3.  **Архитектура Training Pipeline.**
4.  **Архитектура Inference Pipeline (Serving).**

### Часть 3: Расчёты и нефункциональные требования (Мини-отчёт)

1.  **Расчёт требований к хранилищу.**
2.  **Расчёт требований к пропускной способности (Throughput).**
3.  **Масштабируемость и надёжность.**

## 3. Формат сдачи

- **Финальный документ:** Единый Markdown (`.md`) файл.
- **Структура документа:**
    1.  Титульный лист.
    2.  Часть 1: Формулировка ML-задачи.
    3.  Часть 2: Проектирование архитектуры (с диаграммами).
    4.  Часть 3: Расчёты.
    5.  Список источников.

- **Диаграммы:** Экспортируйте из draw.io в PNG и вставьте в файл.

## 4. Критерии оценки

- Полнота и глубина проработки.
- Обоснованность решений.
- Качество диаграмм.
- Корректность расчётов.
- Структура и оформление.
